# -*- coding: utf-8 -*-
"""00_TENSORFLOW_FUNDAMENTALS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lM9XlX6S1E18bcpH5kP6oeMEJvoO_3P-

# **TensorFlow Fundamentals**

## **Introduction to Tensors**
"""

#Import tensorFlow
import tensorflow as tf
print(tf.__version__)

#create tensors with tf.constant()
scalar = tf.constant(7)
scalar

#check the number of dimensions of a tensor(ndim stands for number of dimensions)
scalar.ndim

#create a vector
vector = tf.constant([10,10])
vector

#check the dimension of our vector
vector.ndim

#create a matrix (has more than 1 dimension)
matrix = tf.constant([[10,7],[7,10]])
matrix

matrix.ndim

#create another matrix
another_matrix = tf.constant([[10.,7.],[3.,2.],[8.,9.]], dtype=tf.float16) #specify the data type with dtype parameter
another_matrix

another_matrix.ndim

#Let's create a tensor
tensor = tf.constant([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]],[[13,14,15],[16,17,18]]])
tensor

tensor.ndim

"""### **what we created so far*

Scalar: a single number

Vector: a number with direction (e.g. wind speed and direction)

Matrix: a 2-dimensional array of numbers

Tensor: an n-dimensional array of numbers (when n can be any number, a 0-dimensional is a scalar, a 1-dimensional is a vector)

# **Creating tensors with tf.Variable**
"""

#Create the same tensor with tf.variable as above
changeable_tensor = tf.Variable([10,7])
unchangeable_tensor = tf.constant([10,7])
changeable_tensor,unchangeable_tensor

#let's try to change one of the elements in our changable tensor
changeable_tensor[0] = 7
changeable_tensor

#How about we try assign
changeable_tensor[0].assign(7)
changeable_tensor

#let's try to change unchangeable tensor
unchangeable_tensor[0].assign(7)
unchangeable_tensor

"""# **Creating Random tensors**

## Random tensors are tensors of some arbitary size which contain random numbers.
"""

#Create two random (but the same) tensors
random_1 = tf.random.Generator.from_seed(42) #set seed for reproducibility
random_1 = random_1.normal(shape=(3,2))
random_2 = tf.random.Generator.from_seed(42)
random_2 = random_2.normal(shape=(3,2))

#Are they equal?
random_1,random_2,random_1 == random_2

"""# **Shuffle the order of elements in a tensor**"""

#shuffle a tensor (valuable for when you want to shuffle your data so the inherent order doesn't effect learning)
not_shuffled = tf.constant([[10,7],[3,4],[2,5]])
#shuffle our non-shuffled tensor
tf.random.shuffle(not_shuffled)

#shuffle our non-shuffled tensor
tf.random.set_seed(42)
tf.random.shuffle(not_shuffled,seed=42)

not_shuffled

"""# **Create 2 random tensors and shuffle it**"""

r1 = tf.random.Generator.from_seed(41)
r1 = r1.normal(shape=(3,2))
r1

r2 = tf.random.Generator.from_seed(40)
r2 = r2.normal(shape=(3,2))
r2

tf.random.shuffle(r1)

tf.random.set_seed(40) #global level
tf.random.shuffle(r2,seed=40) #operationl level

"""## **Other ways to make tensors**"""

#Create a tensor of all ones
tf.ones([10,7])

#Create a tensor of all zeroes
tf.zeros(shape=(3,4))

"""### Turn NUMPY arrrays into tensors

The main difference btw numpy arrays and tensorflow tensors is that tensors can be run on a GPU
"""

#You can also turn Numpy arrays into tensors
import numpy as np
numpy_A = np.arange(1,25,dtype=np.int32) #create a numpy array btw 1 and 25
numpy_A
#x = tf.constant(some_matrix)  #capital for matrix or vector
#y = tf.constant(vector) #non-capital for vector

A = tf.constant(numpy_A,shape=(2,3,4))
B = tf.constant(numpy_A)
A,B

2*2*4

A.ndim

"""## Getting Information from tensors

when dealing with tensors youu probably want to be aware of the following attributes
* Shape -- tf.shape
* Rank -- tf.ndim
* Axis or Dimension -- tf[0],tf[:,1]
* Size -- tf.size(tensor)
"""

#Create a rank 4 tensor (4-dimensions)
rank_4_tensor = tf.zeros(shape=[2,3,4,5])
rank_4_tensor

rank_4_tensor[0]

rank_4_tensor.shape,  rank_4_tensor.ndim, tf.size(rank_4_tensor)

2*3*4*5

#Get various attributes of our tensor
print("Datatype of every element:", rank_4_tensor.dtype)
print("Number of dimensions (rank):", rank_4_tensor.ndim)
print("shape of tensor:", rank_4_tensor.shape)
print("Elements along the 0 axis:",rank_4_tensor.shape[0])
print("Elements along the last axis",rank_4_tensor.shape[-1])
print("Total number of elements in our tensor:", tf.size(rank_4_tensor))
print("Total number of elements in our tensor:", tf.size(rank_4_tensor).numpy())

"""###Indexing Tensors

Tensors can be indexed just like python
"""

#Get the first 2 elements
some_list = [1,2,3,4]
some_list[:2]

#Get the first 2 elements of each dimension
rank_4_tensor[:2,:2,:2,:2]

some_list[:1]

rank_4_tensor.shape

#Get the first element from each dimension from each index except for the final one
rank_4_tensor[:1,:1,:1,:1]

#create a rank 2 tensor (2 dimensions)
rank_2_tensor = tf.constant([[10,7],[3,4]])
rank_2_tensor.shape,rank_2_tensor.ndim

rank_2_tensor

some_list,some_list[-1]

#Get the last item of each row of our rank 2 tensor
rank_2_tensor[:,-1]

#Add in extra dimension to our rank 2 tensor
rank_3_tensor = rank_2_tensor[...,tf.newaxis]
#rank_3_tensor = rank_2_tensor[:,:,tf.newaxis] --- outputs the same
rank_3_tensor

#Alterntive to tf.newaxis
tf.expand_dims(rank_2_tensor,axis=-1) #-1 means expand the final axis

tf.expand_dims(rank_2_tensor,axis=0) #expand the 0 axis

rank_2_tensor

"""# **Manipulating Tensors (tensor operations)**

**Basic operations**

+,-,*,/
"""

#you can add vaalues to a tensor using the addition operator
tensor = tf.constant([[10,7],[3,4]])
tensor+10

#Original tensor is unchanged
tensor

#Multiplicatin also works
tensor*10

#Subtraction
tensor-10

#We can use the tensorflow built-in function too
tf.multiply(tensor,10)

tensor

"""# **Matrix Multiplication**

In machine learning matrix multiplication is one of the common tensor operations.

There are two rules our tensors (or matrices) need to fulfi if we're going to matrix multiply them:

1. The inner dimensions must match
2. The resulting matrix has the shape of the inner dimensions
"""

#Matrix Multiplication
print(tensor)
tf.matmul(tensor,tensor)

tensor*tensor

#Matrix Multiplication wth python operator "@"
tensor @ tensor

tensor.shape

import tensorflow as tf
print(tf.__version__)

#Create a (3,2) tensor
X = tf.constant([[1,2],[3,4],[5,6]])
#Create another (3,2) tensor
Y = tf.constant([[7,8],[9,10],[11,12]])

X,Y

#Try to matrix multiply tensors of same shape
X@Y

#Let's change the shape of Y
tf.reshape(Y,shape=(2,3)).shape

#Try to matrix multiply X by reshaped Y
X @ tf.reshape(Y,shape=(2,3))

X

tf.reshape(Y,shape=(2,3))

tf.matmul(X,tf.reshape(Y,shape=(2,3)))

Y

tf.reshape(X,shape=(2,3))

#Try change the shape of X instead of Y
tf.matmul(tf.reshape(X,shape=(2,3)),Y)

#Can do the same with transpose
X, tf.transpose(X),tf.reshape(X,shape=(2,3))

Y

#Try matrix multiplication with transpose rather than reshape
tf.matmul(tf.transpose(X),Y)

"""** The dot product **
Matrix Multipliction is also referred to as the dot product.

You can perfom matrix multiplication using:
* `tf.matmul()`
* `tf.tesnordot()`
* `@`
"""

#Perform marix multiplication between X and Y (transposed)
tf.matmul(X,tf.transpose(Y))

tf.tensordot(X,tf.transpose(Y),axes=1)

tf.matmul(X,tf.reshape(Y,shape=(2,3)))

#Check the values of Y, reshape Y and transposed Y
print("Normal Y:")
print()
print("Y reshaped to (2,3): ")
print(tf.reshape(Y,(2,3)),"\n")
print("Y transposed:")
print(tf.transpose(Y))

tf.matmul(X,tf.transpose(Y))

"""Generally, when perorming matrixx multiplication on two tensors and one of the axes doesn't line up, you will transpose (rather than reshape) one off the tensors to get satisfy the matrix muliplication rules

# **Changing the datatype of a tensor**
"""

#Create a new tensor with default datatype
B = tf.constant([1.7,7.4])
B.dtype

c = tf.constant([7,10])
c.dtype

#Change from float32 to float16
D = tf.cast(B,dtype=tf.float16)
D, D.dtype

#Change from int32 to float32
E = tf.cast(c,dtype=tf.float32)
E,E.dtype

# Change from float32 to flot16
E_float = tf.cast(E,dtype=tf.float16)
E_float,E_float.dtype

"""# **Aggregating tensors**
Aggregating tensors = considering them from multiple values down to a smaller amount of values
"""

# Get the absolute values
D = tf.constant([-7,-10])
D

# Get the absolute values
tf.abs(D)

"""Let's go through the followwing forms of aggregation:
* Get the minimum
* Get the maximum
* Get the mean of a tensor
* Get the sum of a tensor
"""

# Create a random tensor with values between 0 and 100 of size 50
import numpy as np
E = tf.constant(np.random.randint(0,100,size=50))
E

tf.size(E),E.shape,E.ndim

#Find the minimum
tf.reduce_min(E)

#Find the max
tf.reduce_max(E)

#Find the mean
tf.reduce_mean(E)

# Find the sum
tf.reduce_sum(E)

"""FIND STANDARD DEVIATION AND VARIANCE"""

#To find the variance of our tensor, we need access of tensorflow_probability
import tensorflow_probability as tfp
tfp.stats.variance(E)

# Find the variance
E_float = tf.cast(E,dtype=tf.float32)
tf.math.reduce_variance(E_float)

#Find the variance of our E tensor in single line
tf.math.reduce_variance(tf.cast(E,dtype=tf.float32))

# Find the standard deviation
E_float = tf.cast(E,dtype=tf.float32)
tf.math.reduce_std(E_float)

"""## Find the positional maximum and minimum"""

#Create a new tensor for findng positional minimum and maximum
tf.random.set_seed(42)
F = tf.random.uniform(shape=[50])
F

# Find the positinal maximum
tf.argmax(F)

# Index on our largest value position
F[tf.argmax(F)]

# Find the max value of F
 tf.reduce_max(F)

#Check the equality
F[tf.argmax(F)] == tf.reduce_max(F)

#Find the positional minimum
tf.argmin(F)

#Find the minmum using the positional minimum index
F[tf.argmin(F)]

"""### **Squeezing a tensor (remove all single dimensions)**"""

#Create a tensor to get started
tf.random.set_seed(42)
G = tf.constant(tf.random.uniform(shape=[50]),shape=(1,1,1,1,50))
G

G.shape

G_squeezed = tf.squeeze(G)
G_squeezed,G_squeezed.shape

"""## **One hot encoding**
**Tensorflow stages:**
Inputs-->Numerical encoding-->learns representation(patterns/features/weights)-->Representation outputs-->outputs

One hot encoding is a form of numerical encoding.

Imagine you have a list of fruits: apples, bananas, and oranges. One hot encoding is a way to represent each fruit in a binary format.
First, you create a list of all possible fruits.
Then, for each fruit in your original list, you create a new list with the same length as the list of all possible fruits.
In this new list, you put a 1 in the position that corresponds to the fruit's index in the list of all possible fruits, and 0s in all other positions.
For example:

If your original list is [apples, bananas, oranges], and you have a list of all possible fruits as [apples, bananas, oranges, grapes], then:
Apples would be represented as [1, 0, 0, 0]
Bananas would be represented as [0, 1, 0, 0]
Oranges would be represented as [0, 0, 1, 0]
This way, each fruit is represented uniquely, and the model can easily understand and work with this representation.
"""

#Create a list of indices
 some_list = [0,1,2,3] # could be red, green, blue, purple

 # One hot encode our list of indices
 tf.one_hot(some_list,depth=4)

# Specify custom values for one hot encoding instead of ones and zeroes
tf.one_hot(some_list,depth=4,on_value="yo I love deep learning",off_value="I also like to dance")

"""# **Squaring, Log, Square root**"""

#Create a new tensor
H = tf.range(1,10)
H

#Square it
tf.square(H)

#Find the square root (will error to overcome this method requires non-int type)
tf.sqrt(tf.cast(H,dtype=tf.float32))

#Find the log
tf.math.log(tf.cast(H,dtype=tf.float32))

"""### **Tensors and Numpy**

Tensorflow interacts beautifully with numpy arrays.

Numpy is the fundamental package for scientific computing with python.

Note: One of the main differences between a Tensorflow tensor and a Numpy array is that a TensorFlow tensor can be run on a GPU or TPU
(for faSt numerical processing).
"""

#Create a tensor directly from a Numpy array
J = tf.constant(np.array([3.,7.,10.]))
J

#Convert our tensor back to Numpy array
np.array(J), type(np.array(J))

#Convert tensor J to a Numpy array
J.numpy(),type(J.numpy())

# The default types of each are slightly different
numpy_J = tf.constant(np.array([3.,7.,10.]))
tensor_J = tf.constant([3.,7.,10.])
# Check the datatypes of each
numpy_J.dtype,tensor_J.dtype

